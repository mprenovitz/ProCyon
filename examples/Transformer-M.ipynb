{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ce46fb",
   "metadata": {},
   "source": [
    "# Ligand embedding generation\n",
    "\n",
    "In this notebook, we use Transformer-M to generate embeddings for small molecule ligands. First, we obtain canonical SMILES (2D) and 3D structure data for those ligands from DrugBank. We then wrap them into dataset format compatible with Transformer-M, before running inference with pretrained Transformer-M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install rdkit omegaconf hydra-core bitarray cython python-algos ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0094bc50-cfaa-4922-9405-603a715536af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from torch_geometric.data import Data\n",
    "from rdkit import Chem\n",
    "\n",
    "from procyon.data.data_utils import DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e7222f",
   "metadata": {},
   "source": [
    "## SMILES and 3D structure data\n",
    "\n",
    "If you are interested in generating Transformer-M embeddings for your own data, please format them into a SMILES store and a 3D structure store, similarly as below. Inference can also be performed with only SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "535b75ae-524c-4393-af6b-4dd339b52aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2399\n",
      "3018\n"
     ]
    }
   ],
   "source": [
    "drugbank_drugs = pd.read_pickle(DATA_DIR + \"integrated_data/v1/drugbank/drugbank_info_filtered.pkl\")\n",
    "drugbank_df = pd.read_pickle(DATA_DIR + \"integrated_data/v1/drugbank/raw_drugbank_df.pkl\")\n",
    "drugbank_drugs = drugbank_drugs.merge(drugbank_df[['drugbank_id', 'smiles']], on='drugbank_id')\n",
    "\n",
    "def canon(sm):\n",
    "    try:\n",
    "        return Chem.MolToSmiles(Chem.MolFromSmiles(sm, sanitize=True))\n",
    "    except:\n",
    "        return sm\n",
    "drugbank_drugs['canonical_smiles'] = drugbank_drugs['smiles'].apply(lambda x: canon(x))\n",
    "print((~drugbank_drugs['canonical_smiles'].isna()).sum())\n",
    "print(drugbank_drugs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c7561ff-55c6-496a-b95d-280b5411cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9468\n"
     ]
    }
   ],
   "source": [
    "drugbank_3d_data = Chem.SDMolSupplier(DATA_DIR + \"integrated_data/v1/drugbank/drugbank_3d_structures.sdf\")\n",
    "print(len(drugbank_3d_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3c05d45-270b-45fb-b7ab-c12f30e4bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugbank_drugs_3d = [None] * len(drugbank_drugs)\n",
    "drugbank_id_to_index = drugbank_drugs[['index', 'drugbank_id']].set_index('drugbank_id').to_dict()['index']\n",
    "\n",
    "for mol_3d in drugbank_3d_data:\n",
    "    drugbank_id = mol_3d.GetProp('DATABASE_ID')\n",
    "    if drugbank_id in drugbank_id_to_index.keys():\n",
    "        index = drugbank_id_to_index[drugbank_id]\n",
    "        drugbank_drugs_3d[index] = mol_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07725fa3-dbfb-47e7-8497-62545c2401f2",
   "metadata": {},
   "source": [
    "## Formatting into dataset and inference\n",
    "\n",
    "Please clone the [Transformer-M fork](https://github.com/jasperhyp/Transformer-M/tree/main) before executing the code below.\n",
    "\n",
    "The L18 pretrained weights tensor was downloaded following the Transformer-M instructions. A copy was saved in the corresponding directory as specified in the code below.\n",
    "\n",
    "------------------------------------------------------------------\n",
    "\n",
    "As a reference, we made the following changes to the original repo:\n",
    "1. Renamed the `Transformer-M` source subdirectory (not the project directory) as `Transformer_M`.\n",
    "2. Transformer-M was developed with a very different environment than ProCyon. Instead of setting up a new environment, we patched the Transformer-M codebase so that we can use ProCyon environment to perform inference there. To use the codebase without a new environment, the following files were edited:\n",
    "    1. `Transformer-M/Transformer_M/data/algos.pyx`: Replace all `astype(long,` with `astype(int,`.\n",
    "    2. `Transformer-M/fairseq/modules/__init__.py`: Comment out lines 39, 77, 78\n",
    "    3. `Transformer-M/Transformer_M/tasks/graph_prediction.py`:\n",
    "        - Comment out lines 33, 35-45, 161-end\n",
    "        - Add `from fairseq.dataclass import FairseqDataclass`\n",
    "    4. `Transformer-M/fairseq/data/indexed_dataset.py`: Replace all `np.float` with `float`\n",
    "    5. `Transformer-M/fairseq/__init__.py`: Comment out lines 32-end.\n",
    "    6. `Transformer-M/fairseq/dataclass/initialize.py`\n",
    "        - Add `import dataclasses`\n",
    "        - Replace `v = FairseqConfig.__dataclass_fields__[k].default` with \n",
    "            ```python\n",
    "            field = FairseqConfig.__dataclass_fields__[k]\n",
    "            v = field.default\n",
    "            if v is dataclasses.MISSING and field.default_factory is not dataclasses.MISSING:\n",
    "                v = field.default_factory()\n",
    "            ```\n",
    "    7. `Transformer-M/fairseq/dataclass/configs.py`: Replace all definitions such as `common: CommonConfig = CommonConfig()` in class `FairseqConfig` with `common: CommonConfig = field(default_factory=CommonConfig)`. I.e., instead of defining the default to be an instance, define the default as a `field` of `default_factory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /path/to/Transformer-M; python setup_cython.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0fdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Source folder imported correctly.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "PROJECT_ROOT = Path(\"/path/to/Transformer-M\")\n",
    "\n",
    "# 2. Add this OUTER folder to sys.path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# 3. Import\n",
    "# Now Python finds 'Transformer_M' inside 'PROJECT_ROOT'\n",
    "from Transformer_M.modules.transformer_m_encoder import TransformerMEncoder\n",
    "from Transformer_M.tasks.graph_prediction import GraphPredictionConfig\n",
    "from Transformer_M.data.wrapper import (\n",
    "    smiles2graph,\n",
    "    mol2graph,\n",
    "    preprocess_item,\n",
    ")\n",
    "from Transformer_M.data.collator import collator_3d\n",
    "\n",
    "print(\"Success: Source folder imported correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f1786-0c6e-4953-8e3a-e8d094a6aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocess as mp\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f51d0",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18596ab5-74ad-48bb-aa7b-b46c685fd97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugbank_drugs_w_smiles = drugbank_drugs[~drugbank_drugs['canonical_smiles'].isna()][['drugbank_id', 'canonical_smiles']]\n",
    "drugbank_drugs_w_smiles_3d = dict(zip(drugbank_drugs_w_smiles['drugbank_id'].values, np.array(drugbank_drugs_3d)[~drugbank_drugs['canonical_smiles'].isna()].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e753f41-d03a-4fa1-89a0-ad6119679c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyGDataset(InMemoryDataset):\n",
    "    def __init__(self, root_dir, drugbank_drugs_2d, drugbank_drugs_3d, use_3d=True, smiles2graph=smiles2graph, transform=None, pre_transform=None):\n",
    "        self.drugbank_drugs_2d = drugbank_drugs_2d\n",
    "        self.drugbank_drugs_3d = drugbank_drugs_3d\n",
    "        self.use_3d = use_3d\n",
    "        self.smiles2graph = smiles2graph\n",
    "\n",
    "        super(PyGDataset, self).__init__(root_dir, transform, pre_transform)\n",
    "\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return self.root\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return f'all_molecules_transformer_m.pt'\n",
    "    \n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return self.root\n",
    "\n",
    "    def process(self):\n",
    "        smiles_dict = self.drugbank_drugs_2d.set_index('drugbank_id').to_dict()['canonical_smiles']\n",
    "        graph_pos_dict = self.drugbank_drugs_3d\n",
    "\n",
    "        print('Converting SMILES strings into 2D graphs...')\n",
    "        data_2d_list = []\n",
    "        with mp.Pool(processes=16) as pool:\n",
    "            iter = pool.imap(smiles2graph, list(smiles_dict.values()))\n",
    "\n",
    "            for i, graph in tqdm(enumerate(iter)):\n",
    "                try:\n",
    "                    data = Data()\n",
    "\n",
    "                    assert (len(graph['edge_feat']) == graph['edge_index'].shape[1])\n",
    "                    assert (len(graph['node_feat']) == graph['num_nodes'])\n",
    "\n",
    "                    data.__num_nodes__ = int(graph['num_nodes'])\n",
    "                    data.edge_index = torch.from_numpy(graph['edge_index']).to(torch.int64)\n",
    "                    data.edge_attr = torch.from_numpy(graph['edge_feat']).to(torch.int64)\n",
    "                    data.x = torch.from_numpy(graph['node_feat']).to(torch.int64)\n",
    "                    data.pos = torch.zeros(data.__num_nodes__, 3).to(torch.float32)\n",
    "\n",
    "                    data_2d_list.append(data)\n",
    "                \n",
    "                except:\n",
    "                    data_2d_list.append(None)\n",
    "                    continue\n",
    "        \n",
    "        data_2d_dict = dict(zip(smiles_dict.keys(), data_2d_list))\n",
    "        \n",
    "        print('Extracting 3D positions...')\n",
    "        data_3d_list = []\n",
    "        \n",
    "        with mp.Pool(processes=120) as pool:\n",
    "            iter = pool.imap(mol2graph, list(graph_pos_dict.values()))\n",
    "\n",
    "            for i, graph in tqdm(enumerate(iter), total=len(graph_pos_dict)):\n",
    "                if graph is None:\n",
    "                    data_3d_list.append(None)\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    data = Data()\n",
    "\n",
    "                    assert (len(graph['edge_feat']) == graph['edge_index'].shape[1])\n",
    "                    assert (len(graph['node_feat']) == graph['num_nodes'])\n",
    "\n",
    "                    data.__num_nodes__ = int(graph['num_nodes'])\n",
    "                    data.edge_index = torch.from_numpy(graph['edge_index']).to(torch.int64)\n",
    "                    data.edge_attr = torch.from_numpy(graph['edge_feat']).to(torch.int64)\n",
    "                    data.x = torch.from_numpy(graph['node_feat']).to(torch.int64)\n",
    "                    data.pos = torch.from_numpy(graph['position']).to(torch.float32)\n",
    "\n",
    "                    data_3d_list.append(data)\n",
    "                    \n",
    "                except:\n",
    "                    data_3d_list.append(None)\n",
    "                    continue\n",
    "        \n",
    "        data_3d_dict = dict(zip(graph_pos_dict.keys(), data_3d_list))\n",
    "        data_dict = dict((k, v) if v is not None else (k, data_2d_dict[k]) for k, v in data_3d_dict.items())\n",
    "        for k, v in list(data_dict.items()):\n",
    "            if v is None:\n",
    "                data_dict.pop(k)\n",
    "                data_2d_dict.pop(k)\n",
    "                data_3d_dict.pop(k)\n",
    "        \n",
    "        self.data_3d_dict = data_3d_dict\n",
    "        self.data_2d_dict = data_2d_dict\n",
    "        self.data_dict = data_dict\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_dict.values()]\n",
    "        else:\n",
    "            data_list = list(data_dict.values())\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "\n",
    "        print('Saving...')\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    @lru_cache(maxsize=2048)\n",
    "    def __getitem__(self, idx):\n",
    "        # item = self.get(self.indices()[idx])\n",
    "        item = self.get(idx)\n",
    "        item.idx = idx\n",
    "        return preprocess_item(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7508888-aafe-48bf-b5cc-b9d0ac8ca933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting SMILES strings into 2D graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2399it [00:00, 3145.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 3D positions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2399/2399 [00:00<00:00, 3024.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "all_mols_store = PyGDataset(\n",
    "    root_dir=DATA_DIR+\"integrated_data/v1/drugbank/\", \n",
    "    drugbank_drugs_2d=drugbank_drugs_w_smiles, \n",
    "    drugbank_drugs_3d=drugbank_drugs_w_smiles_3d, \n",
    "    use_3d=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990ba4f",
   "metadata": {},
   "source": [
    "### Load model and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f700e920-6e04-4ec6-8482-bf1d800abb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GraphPredictionConfig  # do not change since the pretrained models are trained with this config\n",
    "multi_hop_max_dist = 5\n",
    "spatial_pos_max = 1024\n",
    "\n",
    "# config.num_atoms = 512*9\n",
    "# config.num_in_degree = 512\n",
    "# config.num_out_degree = 512\n",
    "# config.num_edges = 512*3\n",
    "# config.num_spatial = 512\n",
    "# config.num_edge_dis = 128\n",
    "# config.edge_type = \"multihop\"\n",
    "config.multi_hop_max_dist = multi_hop_max_dist\n",
    "config.encoder_layers = 18\n",
    "config.encoder_embed_dim = 768\n",
    "config.encoder_ffn_embed_dim = 768 \n",
    "config.encoder_attention_heads = 32 \n",
    "config.dropout = 0.0\n",
    "config.attention_dropout = 0.1 \n",
    "config.act_dropout = 0.1\n",
    "# config.max_positions = 512\n",
    "config.num_segment = 2\n",
    "config.no_token_positional_embeddings = False\n",
    "config.encoder_normalize_before = True\n",
    "config.apply_init = True\n",
    "config.activation_fn = \"gelu\"\n",
    "config.encoder_learned_pos = True\n",
    "config.sandwich_ln = False\n",
    "config.droppath_prob = 0.1\n",
    "config.add_3d = True\n",
    "config.num_3d_bias_kernel = 128\n",
    "config.mode_prob = \"0.2,0.2,0.6\"\n",
    "config.no_2d = False\n",
    "# config.noise_scale = 0.2  # 0.01\n",
    "# config.criterion = \"graph_prediction\"\n",
    "# config.arch = \"transformer_m_base\"\n",
    "\n",
    "mol_encoder = TransformerMEncoder(\n",
    "    num_atoms=config.num_atoms,\n",
    "    num_in_degree=config.num_in_degree,\n",
    "    num_out_degree=config.num_out_degree,\n",
    "    num_edges=config.num_edges,\n",
    "    num_spatial=config.num_spatial,\n",
    "    num_edge_dis=config.num_edge_dis,\n",
    "    edge_type=config.edge_type,\n",
    "    multi_hop_max_dist=config.multi_hop_max_dist,\n",
    "    num_encoder_layers=config.encoder_layers,\n",
    "    embedding_dim=config.encoder_embed_dim,\n",
    "    ffn_embedding_dim=config.encoder_ffn_embed_dim,\n",
    "    num_attention_heads=config.encoder_attention_heads,\n",
    "    dropout=config.dropout,\n",
    "    attention_dropout=config.attention_dropout,\n",
    "    activation_dropout=config.act_dropout,\n",
    "    max_seq_len=config.max_positions,\n",
    "    num_segments=config.num_segment,\n",
    "    use_position_embeddings=not config.no_token_positional_embeddings,\n",
    "    encoder_normalize_before=config.encoder_normalize_before,\n",
    "    apply_init=config.apply_init,\n",
    "    activation_fn=config.activation_fn,\n",
    "    learned_pos_embedding=config.encoder_learned_pos,\n",
    "    sandwich_ln=config.sandwich_ln,\n",
    "    droppath_prob=config.droppath_prob,\n",
    "    add_3d=config.add_3d,\n",
    "    num_3d_bias_kernel=config.num_3d_bias_kernel,\n",
    "    no_2d=config.no_2d,\n",
    "    mode_prob=config.mode_prob,\n",
    ")\n",
    "\n",
    "mol_encoder_state_dict = torch.load(os.path.join(DATA_DIR, \"model_weights/L18\"))[\"model\"]\n",
    "remove_keys = []\n",
    "for k, v in list(mol_encoder_state_dict.items()):  # create a copy of the items to avoid changing the original dict size while iterating itself\n",
    "    if 'molecule_encoder' not in k:\n",
    "        pass\n",
    "    else:\n",
    "        mol_encoder_state_dict[k[len('encoder.molecule_encoder.'):]] = v\n",
    "    remove_keys.append(k)\n",
    "\n",
    "for k in remove_keys: \n",
    "    mol_encoder_state_dict.pop(k)\n",
    "mol_encoder.load_state_dict(mol_encoder_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0559e50d-8c43-4f58-97e3-23f1654615a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "for start in range(0, len(all_mols_store), batch_size):\n",
    "    raw_batch_mols = []\n",
    "\n",
    "    # Dummy label to avoid crash in collator\n",
    "    for i in range(start, start + batch_size):\n",
    "        item = all_mols_store[i]\n",
    "        item.y = torch.tensor([0.0]) \n",
    "        raw_batch_mols.append(item)\n",
    "    raw_batch_mols = tuple(raw_batch_mols)\n",
    "    \n",
    "    # raw_batch_mols = tuple(all_mols_store[i] for i in range(start, start+batch_size))\n",
    "    batch_mols = collator_3d(raw_batch_mols, max_node=100000, multi_hop_max_dist=multi_hop_max_dist, spatial_pos_max=spatial_pos_max)\n",
    "    temp, _ = mol_encoder(batch_mols, last_state_only=True)\n",
    "    assert len(temp) == 1\n",
    "    all_embeddings.append(temp[0][0, :, :].detach().cpu())  # [SEQ, BATCH, FEAT]\n",
    "\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50d1b405-ce2f-4e85-84ee-f24c5cef16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugbank_ids = list(all_mols_store.data_dict.keys())\n",
    "drug_indices = [drugbank_id_to_index[drugbank_id] for drugbank_id in drugbank_ids]\n",
    "input_3d = [all_mols_store.data_3d_dict[drugbank_id] is not None for drugbank_id in drugbank_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a708ae-f43d-4fe9-82a4-a0a5c5237595",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"embeds\": all_embeddings,\n",
    "    \"drugbank_ids\": drugbank_ids,\n",
    "    \"drugbank_indices\": drug_indices,\n",
    "    \"input_3d\": input_3d,\n",
    "}, DATA_DIR+\"integrated_data/v1/drugbank/drugbank_compound_embeddings_transformer_m_18.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procyon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
